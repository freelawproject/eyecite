<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>eyecite.tokenizers API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>eyecite.tokenizers</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import hashlib
import re
from collections import defaultdict
from copy import deepcopy
from dataclasses import dataclass, field
from pathlib import Path
from string import Template
from typing import (
    Any,
    Generator,
    Iterable,
    List,
    Optional,
    Sequence,
    Set,
    Tuple,
)

import ahocorasick
from reporters_db import JOURNALS, LAWS, RAW_REGEX_VARIABLES, REPORTERS
from reporters_db.utils import process_variables, recursive_substitute

from eyecite.models import (
    CitationToken,
    Edition,
    IdToken,
    ParagraphToken,
    Reporter,
    SectionToken,
    StopWordToken,
    SupraToken,
    Token,
    TokenExtractor,
    Tokens,
)
from eyecite.regexes import (
    ID_REGEX,
    PAGE_NUMBER_REGEX,
    PARAGRAPH_REGEX,
    SECTION_REGEX,
    STOP_WORD_REGEX,
    STOP_WORDS,
    SUPRA_REGEX,
    nonalphanum_boundaries_re,
    short_cite_re,
)

# Prepare extractors

# An extractor is an object that applies a particular regex to a string
# and returns Tokens for each match. We need to build a list of all of
# our extractors. Also build a lookup of Editions by reporter string,
# though that isn&#39;t directly used outside of tests.

EXTRACTORS = []
EDITIONS_LOOKUP = defaultdict(list)


def _populate_reporter_extractors():
    &#34;&#34;&#34;Populate EXTRACTORS and EDITIONS_LOOKUP.&#34;&#34;&#34;

    # Set up regex replacement variables from reporters-db
    raw_regex_variables = deepcopy(RAW_REGEX_VARIABLES)
    raw_regex_variables[&#34;full_cite&#34;][&#34;&#34;] = &#34;$volume $reporter,? $page&#34;
    raw_regex_variables[&#34;page&#34;][&#34;&#34;] = rf&#34;(?P&lt;page&gt;{PAGE_NUMBER_REGEX})&#34;
    regex_variables = process_variables(raw_regex_variables)

    def _substitute_edition(template, *edition_names):
        &#34;&#34;&#34;Helper to replace $edition in template with edition_names.&#34;&#34;&#34;
        edition = &#34;|&#34;.join(re.escape(e) for e in edition_names)
        return Template(template).safe_substitute(edition=edition)

    # Extractors step one: add an extractor for each reporter string

    # Build a lookup of regex -&gt; edition.
    # Keys in this dict will be regular expressions to handle a
    # particular reporter string, like (simplified)
    # r&#34;(?P&lt;volume&gt;\d+) (?P&lt;reporter&gt;U\.S\.) (?P&lt;page&gt;\d+)&#34;
    editions_by_regex = defaultdict(
        # Values in this dict will be:
        lambda: {
            # Exact matches. If the regex is &#34;\d+ U.S. \d+&#34;,
            # this will be [Edition(&#34;U.S.&#34;)]
            &#34;editions&#34;: [],
            # Variants. If the regex matches &#34;\d+ U. S. \d+&#34;,
            # this will be [Edition(&#34;U.S.&#34;)]
            &#34;variations&#34;: [],
            # Strings a text must contain for this regex to match.
            # If the regex is &#34;\d+ S.E. 2d \d+&#34;,
            # this will be {&#34;S.E. 2d&#34;}
            &#34;strings&#34;: set(),
            # Whether this regex results in a short cite:
            &#34;short&#34;: False,
        }
    )

    def _add_regex(
        kind: str,
        reporters: List[str],
        edition: Edition,
        regex: str,
    ):
        &#34;&#34;&#34;Helper to generate citations for a reporter
        and insert into editions_by_regex.&#34;&#34;&#34;
        for reporter in reporters:
            EDITIONS_LOOKUP[reporter].append(edition)
        editions_by_regex[regex][kind].append(edition)

        # add strings
        have_strings = re.escape(reporters[0]) in regex
        if have_strings:
            editions_by_regex[regex][&#34;strings&#34;].update(reporters)

        # add short cite
        short_cite_regex = short_cite_re(regex)
        if short_cite_regex != regex:
            editions_by_regex[short_cite_regex][kind].append(edition)
            editions_by_regex[short_cite_regex][&#34;short&#34;] = True
            if have_strings:
                editions_by_regex[short_cite_regex][&#34;strings&#34;].update(
                    reporters
                )

    def _add_regexes(
        regex_templates: List[str],
        edition_name: str,
        edition: Edition,
        variations: List[str],
    ):
        &#34;&#34;&#34;Expand regex_templates and add to editions_by_regex.&#34;&#34;&#34;
        for regex_template in regex_templates:
            regex_template = recursive_substitute(
                regex_template, regex_variables
            )
            regex = _substitute_edition(regex_template, edition_name)
            _add_regex(&#34;editions&#34;, [edition_name], edition, regex)
            if variations:
                regex = _substitute_edition(regex_template, *variations)
                _add_regex(
                    &#34;variations&#34;,
                    variations,
                    edition,
                    regex,
                )

    # add reporters.json:
    for source_key, source_cluster in REPORTERS.items():
        for source in source_cluster:
            reporter_obj = Reporter(
                short_name=source_key,
                name=source[&#34;name&#34;],
                cite_type=source[&#34;cite_type&#34;],
                source=&#34;reporters&#34;,
            )
            variations = source[&#34;variations&#34;]
            for edition_name, edition_data in source[&#34;editions&#34;].items():
                edition = Edition(
                    short_name=edition_name,
                    reporter=reporter_obj,
                    start=edition_data[&#34;start&#34;],
                    end=edition_data[&#34;end&#34;],
                )
                regex_templates = edition_data.get(&#34;regexes&#34;) or [&#34;$full_cite&#34;]
                edition_variations = [
                    k for k, v in variations.items() if v == edition_name
                ]
                _add_regexes(
                    regex_templates, edition_name, edition, edition_variations
                )

    # add laws.json
    for source_key, source_cluster in LAWS.items():
        for source in source_cluster:
            reporter_obj = Reporter(
                short_name=source_key,
                name=source[&#34;name&#34;],
                cite_type=source[&#34;cite_type&#34;],
                source=&#34;laws&#34;,
            )
            edition = Edition(
                short_name=source_key,
                reporter=reporter_obj,
                start=source[&#34;start&#34;],
                end=source[&#34;end&#34;],
            )
            regex_templates = source.get(&#34;regexes&#34;) or [&#34;$full_cite&#34;]
            # handle citation to multiple sections, like
            # &#34;Mass. Gen. Laws ch. 1, §§ 2-3&#34;:
            regex_templates = [
                r.replace(r&#34;§ &#34;, r&#34;§§? ?&#34;) for r in regex_templates
            ]
            _add_regexes(
                regex_templates,
                source_key,
                edition,
                source.get(&#34;variations&#34;, []),
            )

    # add journals.json
    for source_key, source_cluster in JOURNALS.items():
        for source in source_cluster:
            reporter_obj = Reporter(
                short_name=source_key,
                name=source[&#34;name&#34;],
                cite_type=source[&#34;cite_type&#34;],
                source=&#34;journals&#34;,
            )
            edition = Edition(
                short_name=source_key,
                reporter=reporter_obj,
                start=source[&#34;start&#34;],
                end=source[&#34;end&#34;],
            )
            regex_templates = source.get(&#34;regexes&#34;) or [&#34;$full_cite&#34;]
            _add_regexes(
                regex_templates,
                source_key,
                edition,
                source.get(&#34;variations&#34;, []),
            )

    # Add each regex to EXTRACTORS:
    for regex, cluster in editions_by_regex.items():
        EXTRACTORS.append(
            TokenExtractor(
                nonalphanum_boundaries_re(regex),
                CitationToken.from_match,
                extra={
                    &#34;exact_editions&#34;: cluster[&#34;editions&#34;],
                    &#34;variation_editions&#34;: cluster[&#34;variations&#34;],
                    &#34;short&#34;: cluster[&#34;short&#34;],
                },
                strings=list(cluster[&#34;strings&#34;]),
            )
        )

    # Extractors step two:
    # Add a few one-off extractors to handle special token types
    # other than citations:

    EXTRACTORS.extend(
        [
            # Id.
            TokenExtractor(
                ID_REGEX,
                IdToken.from_match,
                flags=re.I,
                strings=[&#34;id.&#34;, &#34;ibid.&#34;],
            ),
            # supra
            TokenExtractor(
                SUPRA_REGEX,
                SupraToken.from_match,
                flags=re.I,
                strings=[&#34;supra&#34;],
            ),
            # paragraph
            TokenExtractor(
                PARAGRAPH_REGEX,
                ParagraphToken.from_match,
            ),
            # case name stopwords
            TokenExtractor(
                STOP_WORD_REGEX,
                StopWordToken.from_match,
                flags=re.I,
                strings=STOP_WORDS,
            ),
            # tokens containing section symbols
            TokenExtractor(
                SECTION_REGEX, SectionToken.from_match, strings=[&#34;§&#34;]
            ),
        ]
    )


_populate_reporter_extractors()

# Tokenizers


@dataclass
class Tokenizer:
    &#34;&#34;&#34;A tokenizer takes a list of extractors, and provides a tokenize()
    method to tokenize text using those extractors.
    This base class should be overridden by tokenizers that use a
    more efficient strategy for running all the extractors.&#34;&#34;&#34;

    extractors: List[TokenExtractor] = field(
        default_factory=lambda: list(EXTRACTORS)
    )

    def tokenize(self, text: str) -&gt; Tuple[Tokens, List[Tuple[int, Token]]]:
        &#34;&#34;&#34;Tokenize text and return list of all tokens, followed by list of
        just non-string tokens along with their positions in the first list.&#34;&#34;&#34;
        # Sort all matches by start offset ascending, then end offset
        # descending. Remove overlaps by returning only matches
        # where the current start offset is greater than the previously
        # returned end offset. Also return text between matches.
        citation_tokens = []
        all_tokens: Tokens = []
        tokens = sorted(
            self.extract_tokens(text), key=lambda m: (m.start, -m.end)
        )
        last_token = None
        offset = 0
        for token in tokens:
            if last_token:
                # Sometimes the exact same cite is matched by two different
                # regexes. Attempt to merge rather than discarding one or the
                # other:
                merged = last_token.merge(token)
                if merged:
                    continue
            if offset &gt; token.start:
                # skip overlaps
                continue
            if offset &lt; token.start:
                # capture plain text before each match
                self.append_text(all_tokens, text[offset : token.start])
            # capture match
            citation_tokens.append((len(all_tokens), token))
            all_tokens.append(token)
            offset = token.end
            last_token = token
        # capture plain text after final match
        if offset &lt; len(text):
            self.append_text(all_tokens, text[offset:])
        return all_tokens, citation_tokens

    def get_extractors(self, text: str):
        &#34;&#34;&#34;Subclasses can override this to filter extractors based on text.&#34;&#34;&#34;
        return self.extractors

    def extract_tokens(self, text) -&gt; Generator[Token, None, None]:
        &#34;&#34;&#34;Get all instances where an extractor matches the given text.&#34;&#34;&#34;
        for extractor in self.get_extractors(text):
            for match in extractor.get_matches(text):
                yield extractor.get_token(match)

    @staticmethod
    def append_text(tokens, text):
        &#34;&#34;&#34;Split text into words, treating whitespace as a word, and append
        to tokens. NOTE this is a significant portion of total runtime of
        get_citations(), so benchmark if changing.
        &#34;&#34;&#34;
        for part in text.split(&#34; &#34;):
            if part:
                tokens.extend((part, &#34; &#34;))
            else:
                tokens.append(&#34; &#34;)
        tokens.pop()  # remove final extra space


@dataclass
class AhocorasickTokenizer(Tokenizer):
    &#34;&#34;&#34;A performance-optimized Tokenizer using the
    pyahocorasick library. Only runs extractors where
    the target text contains one of the strings from
    TokenExtractor.strings.&#34;&#34;&#34;

    def __post_init__(self):
        &#34;&#34;&#34;Set up helpers to narrow down possible extractors.&#34;&#34;&#34;
        # Build a set of all extractors that don&#39;t list required strings
        self.unfiltered_extractors = set(
            e for e in EXTRACTORS if not e.strings
        )
        # Build a pyahocorasick filter for all case-sensitive extractors
        self.case_sensitive_filter = self.make_ahocorasick_filter(
            (s, e)
            for e in EXTRACTORS
            if e.strings and not e.flags &amp; re.I
            for s in e.strings
        )
        # Build a pyahocorasick filter for all case-insensitive extractors
        self.case_insensitive_filter = self.make_ahocorasick_filter(
            (s.lower(), e)
            for e in EXTRACTORS
            if e.strings and e.flags &amp; re.I
            for s in e.strings
        )

    def get_extractors(self, text: str) -&gt; Set[TokenExtractor]:
        &#34;&#34;&#34;Override get_extractors() to filter out extractors
        that can&#39;t possibly match.&#34;&#34;&#34;
        unique_extractors = set(self.unfiltered_extractors)
        for _, extractors in self.case_sensitive_filter.iter(text):
            unique_extractors.update(extractors)
        for _, extractors in self.case_insensitive_filter.iter(text.lower()):
            unique_extractors.update(extractors)
        return unique_extractors

    @staticmethod
    def make_ahocorasick_filter(
        items: Iterable[Sequence[Any]],
    ) -&gt; ahocorasick.Automaton:
        &#34;&#34;&#34;Given a list of items like
            [[&#39;see&#39;, stop_word_extractor],
             [&#39;see&#39;, another_extractor],
             [&#39;nope&#39;, some_extractor]],
        return a pyahocorasick filter such that
            text_filter.iter(&#39;...see...&#39;)
        yields
            [[stop_word_extractor, another_extractor]].
        &#34;&#34;&#34;
        grouped = defaultdict(list)
        for string, extractor in items:
            grouped[string].append(extractor)

        text_filter = ahocorasick.Automaton()
        for string, extractors in grouped.items():
            text_filter.add_word(string, extractors)
        text_filter.make_automaton()
        return text_filter


@dataclass
class HyperscanTokenizer(Tokenizer):
    &#34;&#34;&#34;A performance-optimized Tokenizer using the
    hyperscan library. Precompiles a database of all
    extractors and runs them in a single pass through
    the target text.&#34;&#34;&#34;

    # Precompiling the database takes several seconds.
    # To avoid that, provide a cache directory writeable
    # only by this user where the precompiled database
    # can be stored.
    cache_dir: Optional[str] = None

    def extract_tokens(self, text) -&gt; Generator[Token, None, None]:
        &#34;&#34;&#34;Extract tokens via hyperscan.&#34;&#34;&#34;
        # Get all matches, with byte offsets because hyperscan uses
        # bytes instead of unicode:
        text_bytes = text.encode(&#34;utf8&#34;)
        matches = []

        def on_match(index, start, end, flags, context):
            matches.append((self.extractors[index], (start, end)))

        self.hyperscan_db.scan(text_bytes, match_event_handler=on_match)

        # Build a lookup table of byte offset -&gt; str offset for all of the
        # matches we found. Stepping through offsets in sorted order avoids
        # having to decode each part of the string more than once:
        byte_to_str_offset = {}
        last_byte_offset = 0
        str_offset = 0
        byte_offsets = sorted(set(i for m in matches for i in m[1]))
        for byte_offset in byte_offsets:
            try:
                str_offset += len(
                    text_bytes[last_byte_offset:byte_offset].decode(&#34;utf8&#34;)
                )
            except UnicodeDecodeError:
                # offsets will fail to decode for invalid regex matches
                # that don&#39;t align with a unicode character
                continue
            byte_to_str_offset[byte_offset] = str_offset
            last_byte_offset = byte_offset

        # Narrow down our matches to only those that successfully decoded,
        # re-run regex against just the matching strings to get match groups
        # (which aren&#39;t provided by hyperscan), and tokenize:
        for extractor, (start, end) in matches:
            if start in byte_to_str_offset and end in byte_to_str_offset:
                start = byte_to_str_offset[start]
                end = byte_to_str_offset[end]
                m = extractor.compiled_regex.match(text[start:end])
                yield extractor.get_token(m, offset=start)

    @property
    def hyperscan_db(self):
        &#34;&#34;&#34;Compile extractors into a hyperscan DB. Use a cache file
        if we&#39;ve compiled this set before.&#34;&#34;&#34;
        if not hasattr(self, &#34;_db&#34;):
            # import here so the dependency is optional
            import hyperscan  # pylint: disable=import-outside-toplevel

            hyperscan_db = None
            cache = None

            flag_conversion = {re.I: hyperscan.HS_FLAG_CASELESS}

            def convert_flags(re_flags):
                hyperscan_flags = 0
                for re_flag, hyperscan_flag in flag_conversion.items():
                    if re_flags &amp; re_flag:
                        hyperscan_flags |= hyperscan_flag
                return hyperscan_flags

            def convert_regex(regex):
                # hyperscan doesn&#39;t understand repetition flags like {,3},
                # so replace with {0,3}:
                regex = re.sub(r&#34;\{,(\d+)\}&#34;, r&#34;{0,\1}&#34;, regex)
                # Characters like &#34;§&#34; convert to more than one byte in utf8,
                # so &#34;§?&#34; won&#39;t work as expected. Convert &#34;§?&#34; to &#34;(?:§)?&#34;:
                long_chars = [c for c in regex if len(c.encode(&#34;utf8&#34;)) &gt; 1]
                if long_chars:
                    regex = re.sub(
                        rf&#39;([{&#34;&#34;.join(set(long_chars))}])\?&#39;, r&#34;(?:\1)?&#34;, regex
                    )
                # encode as bytes:
                return regex.encode(&#34;utf8&#34;)

            expressions = [convert_regex(e.regex) for e in self.extractors]
            # HS_FLAG_SOM_LEFTMOST so hyperscan includes the start offset
            flags = [
                convert_flags(e.flags) | hyperscan.HS_FLAG_SOM_LEFTMOST
                for e in self.extractors
            ]

            if self.cache_dir is not None:
                # Attempt to use cache.
                # Cache key is a hash of all regexes and flags, so we
                # automatically recompile if anything changes.
                fingerprint = hashlib.md5(
                    str(expressions).encode(&#34;utf8&#34;) + str(flags).encode(&#34;utf8&#34;)
                ).hexdigest()
                cache_dir = Path(self.cache_dir)
                cache_dir.mkdir(exist_ok=True)
                cache = cache_dir / fingerprint
                if cache.exists():
                    hyperscan_db = hyperscan.loadb(cache.read_bytes())

            if not hyperscan_db:
                # No cache, so compile database.
                hyperscan_db = hyperscan.Database()
                hyperscan_db.compile(expressions=expressions, flags=flags)
                if cache:
                    cache.write_bytes(hyperscan.dumpb(hyperscan_db))

            self._db = hyperscan_db

        return self._db


default_tokenizer = AhocorasickTokenizer()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="eyecite.tokenizers.AhocorasickTokenizer"><code class="flex name class">
<span>class <span class="ident">AhocorasickTokenizer</span></span>
<span>(</span><span>extractors: List[<a title="eyecite.models.TokenExtractor" href="models.html#eyecite.models.TokenExtractor">TokenExtractor</a>] = &lt;factory&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>A performance-optimized Tokenizer using the
pyahocorasick library. Only runs extractors where
the target text contains one of the strings from
TokenExtractor.strings.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class AhocorasickTokenizer(Tokenizer):
    &#34;&#34;&#34;A performance-optimized Tokenizer using the
    pyahocorasick library. Only runs extractors where
    the target text contains one of the strings from
    TokenExtractor.strings.&#34;&#34;&#34;

    def __post_init__(self):
        &#34;&#34;&#34;Set up helpers to narrow down possible extractors.&#34;&#34;&#34;
        # Build a set of all extractors that don&#39;t list required strings
        self.unfiltered_extractors = set(
            e for e in EXTRACTORS if not e.strings
        )
        # Build a pyahocorasick filter for all case-sensitive extractors
        self.case_sensitive_filter = self.make_ahocorasick_filter(
            (s, e)
            for e in EXTRACTORS
            if e.strings and not e.flags &amp; re.I
            for s in e.strings
        )
        # Build a pyahocorasick filter for all case-insensitive extractors
        self.case_insensitive_filter = self.make_ahocorasick_filter(
            (s.lower(), e)
            for e in EXTRACTORS
            if e.strings and e.flags &amp; re.I
            for s in e.strings
        )

    def get_extractors(self, text: str) -&gt; Set[TokenExtractor]:
        &#34;&#34;&#34;Override get_extractors() to filter out extractors
        that can&#39;t possibly match.&#34;&#34;&#34;
        unique_extractors = set(self.unfiltered_extractors)
        for _, extractors in self.case_sensitive_filter.iter(text):
            unique_extractors.update(extractors)
        for _, extractors in self.case_insensitive_filter.iter(text.lower()):
            unique_extractors.update(extractors)
        return unique_extractors

    @staticmethod
    def make_ahocorasick_filter(
        items: Iterable[Sequence[Any]],
    ) -&gt; ahocorasick.Automaton:
        &#34;&#34;&#34;Given a list of items like
            [[&#39;see&#39;, stop_word_extractor],
             [&#39;see&#39;, another_extractor],
             [&#39;nope&#39;, some_extractor]],
        return a pyahocorasick filter such that
            text_filter.iter(&#39;...see...&#39;)
        yields
            [[stop_word_extractor, another_extractor]].
        &#34;&#34;&#34;
        grouped = defaultdict(list)
        for string, extractor in items:
            grouped[string].append(extractor)

        text_filter = ahocorasick.Automaton()
        for string, extractors in grouped.items():
            text_filter.add_word(string, extractors)
        text_filter.make_automaton()
        return text_filter</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="eyecite.tokenizers.Tokenizer" href="#eyecite.tokenizers.Tokenizer">Tokenizer</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="eyecite.tokenizers.AhocorasickTokenizer.make_ahocorasick_filter"><code class="name flex">
<span>def <span class="ident">make_ahocorasick_filter</span></span>(<span>items: Iterable[Sequence[Any]]) ‑> ahocorasick.Automaton</span>
</code></dt>
<dd>
<div class="desc"><p>Given a list of items like
[['see', stop_word_extractor],
['see', another_extractor],
['nope', some_extractor]],
return a pyahocorasick filter such that
text_filter.iter('&hellip;see&hellip;')
yields
[[stop_word_extractor, another_extractor]].</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def make_ahocorasick_filter(
    items: Iterable[Sequence[Any]],
) -&gt; ahocorasick.Automaton:
    &#34;&#34;&#34;Given a list of items like
        [[&#39;see&#39;, stop_word_extractor],
         [&#39;see&#39;, another_extractor],
         [&#39;nope&#39;, some_extractor]],
    return a pyahocorasick filter such that
        text_filter.iter(&#39;...see...&#39;)
    yields
        [[stop_word_extractor, another_extractor]].
    &#34;&#34;&#34;
    grouped = defaultdict(list)
    for string, extractor in items:
        grouped[string].append(extractor)

    text_filter = ahocorasick.Automaton()
    for string, extractors in grouped.items():
        text_filter.add_word(string, extractors)
    text_filter.make_automaton()
    return text_filter</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="eyecite.tokenizers.AhocorasickTokenizer.get_extractors"><code class="name flex">
<span>def <span class="ident">get_extractors</span></span>(<span>self, text: str) ‑> Set[<a title="eyecite.models.TokenExtractor" href="models.html#eyecite.models.TokenExtractor">TokenExtractor</a>]</span>
</code></dt>
<dd>
<div class="desc"><p>Override get_extractors() to filter out extractors
that can't possibly match.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_extractors(self, text: str) -&gt; Set[TokenExtractor]:
    &#34;&#34;&#34;Override get_extractors() to filter out extractors
    that can&#39;t possibly match.&#34;&#34;&#34;
    unique_extractors = set(self.unfiltered_extractors)
    for _, extractors in self.case_sensitive_filter.iter(text):
        unique_extractors.update(extractors)
    for _, extractors in self.case_insensitive_filter.iter(text.lower()):
        unique_extractors.update(extractors)
    return unique_extractors</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="eyecite.tokenizers.Tokenizer" href="#eyecite.tokenizers.Tokenizer">Tokenizer</a></b></code>:
<ul class="hlist">
<li><code><a title="eyecite.tokenizers.Tokenizer.append_text" href="#eyecite.tokenizers.Tokenizer.append_text">append_text</a></code></li>
<li><code><a title="eyecite.tokenizers.Tokenizer.extract_tokens" href="#eyecite.tokenizers.Tokenizer.extract_tokens">extract_tokens</a></code></li>
<li><code><a title="eyecite.tokenizers.Tokenizer.tokenize" href="#eyecite.tokenizers.Tokenizer.tokenize">tokenize</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="eyecite.tokenizers.HyperscanTokenizer"><code class="flex name class">
<span>class <span class="ident">HyperscanTokenizer</span></span>
<span>(</span><span>extractors: List[<a title="eyecite.models.TokenExtractor" href="models.html#eyecite.models.TokenExtractor">TokenExtractor</a>] = &lt;factory&gt;, cache_dir: Optional[str] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>A performance-optimized Tokenizer using the
hyperscan library. Precompiles a database of all
extractors and runs them in a single pass through
the target text.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class HyperscanTokenizer(Tokenizer):
    &#34;&#34;&#34;A performance-optimized Tokenizer using the
    hyperscan library. Precompiles a database of all
    extractors and runs them in a single pass through
    the target text.&#34;&#34;&#34;

    # Precompiling the database takes several seconds.
    # To avoid that, provide a cache directory writeable
    # only by this user where the precompiled database
    # can be stored.
    cache_dir: Optional[str] = None

    def extract_tokens(self, text) -&gt; Generator[Token, None, None]:
        &#34;&#34;&#34;Extract tokens via hyperscan.&#34;&#34;&#34;
        # Get all matches, with byte offsets because hyperscan uses
        # bytes instead of unicode:
        text_bytes = text.encode(&#34;utf8&#34;)
        matches = []

        def on_match(index, start, end, flags, context):
            matches.append((self.extractors[index], (start, end)))

        self.hyperscan_db.scan(text_bytes, match_event_handler=on_match)

        # Build a lookup table of byte offset -&gt; str offset for all of the
        # matches we found. Stepping through offsets in sorted order avoids
        # having to decode each part of the string more than once:
        byte_to_str_offset = {}
        last_byte_offset = 0
        str_offset = 0
        byte_offsets = sorted(set(i for m in matches for i in m[1]))
        for byte_offset in byte_offsets:
            try:
                str_offset += len(
                    text_bytes[last_byte_offset:byte_offset].decode(&#34;utf8&#34;)
                )
            except UnicodeDecodeError:
                # offsets will fail to decode for invalid regex matches
                # that don&#39;t align with a unicode character
                continue
            byte_to_str_offset[byte_offset] = str_offset
            last_byte_offset = byte_offset

        # Narrow down our matches to only those that successfully decoded,
        # re-run regex against just the matching strings to get match groups
        # (which aren&#39;t provided by hyperscan), and tokenize:
        for extractor, (start, end) in matches:
            if start in byte_to_str_offset and end in byte_to_str_offset:
                start = byte_to_str_offset[start]
                end = byte_to_str_offset[end]
                m = extractor.compiled_regex.match(text[start:end])
                yield extractor.get_token(m, offset=start)

    @property
    def hyperscan_db(self):
        &#34;&#34;&#34;Compile extractors into a hyperscan DB. Use a cache file
        if we&#39;ve compiled this set before.&#34;&#34;&#34;
        if not hasattr(self, &#34;_db&#34;):
            # import here so the dependency is optional
            import hyperscan  # pylint: disable=import-outside-toplevel

            hyperscan_db = None
            cache = None

            flag_conversion = {re.I: hyperscan.HS_FLAG_CASELESS}

            def convert_flags(re_flags):
                hyperscan_flags = 0
                for re_flag, hyperscan_flag in flag_conversion.items():
                    if re_flags &amp; re_flag:
                        hyperscan_flags |= hyperscan_flag
                return hyperscan_flags

            def convert_regex(regex):
                # hyperscan doesn&#39;t understand repetition flags like {,3},
                # so replace with {0,3}:
                regex = re.sub(r&#34;\{,(\d+)\}&#34;, r&#34;{0,\1}&#34;, regex)
                # Characters like &#34;§&#34; convert to more than one byte in utf8,
                # so &#34;§?&#34; won&#39;t work as expected. Convert &#34;§?&#34; to &#34;(?:§)?&#34;:
                long_chars = [c for c in regex if len(c.encode(&#34;utf8&#34;)) &gt; 1]
                if long_chars:
                    regex = re.sub(
                        rf&#39;([{&#34;&#34;.join(set(long_chars))}])\?&#39;, r&#34;(?:\1)?&#34;, regex
                    )
                # encode as bytes:
                return regex.encode(&#34;utf8&#34;)

            expressions = [convert_regex(e.regex) for e in self.extractors]
            # HS_FLAG_SOM_LEFTMOST so hyperscan includes the start offset
            flags = [
                convert_flags(e.flags) | hyperscan.HS_FLAG_SOM_LEFTMOST
                for e in self.extractors
            ]

            if self.cache_dir is not None:
                # Attempt to use cache.
                # Cache key is a hash of all regexes and flags, so we
                # automatically recompile if anything changes.
                fingerprint = hashlib.md5(
                    str(expressions).encode(&#34;utf8&#34;) + str(flags).encode(&#34;utf8&#34;)
                ).hexdigest()
                cache_dir = Path(self.cache_dir)
                cache_dir.mkdir(exist_ok=True)
                cache = cache_dir / fingerprint
                if cache.exists():
                    hyperscan_db = hyperscan.loadb(cache.read_bytes())

            if not hyperscan_db:
                # No cache, so compile database.
                hyperscan_db = hyperscan.Database()
                hyperscan_db.compile(expressions=expressions, flags=flags)
                if cache:
                    cache.write_bytes(hyperscan.dumpb(hyperscan_db))

            self._db = hyperscan_db

        return self._db</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="eyecite.tokenizers.Tokenizer" href="#eyecite.tokenizers.Tokenizer">Tokenizer</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="eyecite.tokenizers.HyperscanTokenizer.cache_dir"><code class="name">var <span class="ident">cache_dir</span> : Optional[str]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="eyecite.tokenizers.HyperscanTokenizer.hyperscan_db"><code class="name">var <span class="ident">hyperscan_db</span></code></dt>
<dd>
<div class="desc"><p>Compile extractors into a hyperscan DB. Use a cache file
if we've compiled this set before.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def hyperscan_db(self):
    &#34;&#34;&#34;Compile extractors into a hyperscan DB. Use a cache file
    if we&#39;ve compiled this set before.&#34;&#34;&#34;
    if not hasattr(self, &#34;_db&#34;):
        # import here so the dependency is optional
        import hyperscan  # pylint: disable=import-outside-toplevel

        hyperscan_db = None
        cache = None

        flag_conversion = {re.I: hyperscan.HS_FLAG_CASELESS}

        def convert_flags(re_flags):
            hyperscan_flags = 0
            for re_flag, hyperscan_flag in flag_conversion.items():
                if re_flags &amp; re_flag:
                    hyperscan_flags |= hyperscan_flag
            return hyperscan_flags

        def convert_regex(regex):
            # hyperscan doesn&#39;t understand repetition flags like {,3},
            # so replace with {0,3}:
            regex = re.sub(r&#34;\{,(\d+)\}&#34;, r&#34;{0,\1}&#34;, regex)
            # Characters like &#34;§&#34; convert to more than one byte in utf8,
            # so &#34;§?&#34; won&#39;t work as expected. Convert &#34;§?&#34; to &#34;(?:§)?&#34;:
            long_chars = [c for c in regex if len(c.encode(&#34;utf8&#34;)) &gt; 1]
            if long_chars:
                regex = re.sub(
                    rf&#39;([{&#34;&#34;.join(set(long_chars))}])\?&#39;, r&#34;(?:\1)?&#34;, regex
                )
            # encode as bytes:
            return regex.encode(&#34;utf8&#34;)

        expressions = [convert_regex(e.regex) for e in self.extractors]
        # HS_FLAG_SOM_LEFTMOST so hyperscan includes the start offset
        flags = [
            convert_flags(e.flags) | hyperscan.HS_FLAG_SOM_LEFTMOST
            for e in self.extractors
        ]

        if self.cache_dir is not None:
            # Attempt to use cache.
            # Cache key is a hash of all regexes and flags, so we
            # automatically recompile if anything changes.
            fingerprint = hashlib.md5(
                str(expressions).encode(&#34;utf8&#34;) + str(flags).encode(&#34;utf8&#34;)
            ).hexdigest()
            cache_dir = Path(self.cache_dir)
            cache_dir.mkdir(exist_ok=True)
            cache = cache_dir / fingerprint
            if cache.exists():
                hyperscan_db = hyperscan.loadb(cache.read_bytes())

        if not hyperscan_db:
            # No cache, so compile database.
            hyperscan_db = hyperscan.Database()
            hyperscan_db.compile(expressions=expressions, flags=flags)
            if cache:
                cache.write_bytes(hyperscan.dumpb(hyperscan_db))

        self._db = hyperscan_db

    return self._db</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="eyecite.tokenizers.HyperscanTokenizer.extract_tokens"><code class="name flex">
<span>def <span class="ident">extract_tokens</span></span>(<span>self, text) ‑> Generator[<a title="eyecite.models.Token" href="models.html#eyecite.models.Token">Token</a>, None, None]</span>
</code></dt>
<dd>
<div class="desc"><p>Extract tokens via hyperscan.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_tokens(self, text) -&gt; Generator[Token, None, None]:
    &#34;&#34;&#34;Extract tokens via hyperscan.&#34;&#34;&#34;
    # Get all matches, with byte offsets because hyperscan uses
    # bytes instead of unicode:
    text_bytes = text.encode(&#34;utf8&#34;)
    matches = []

    def on_match(index, start, end, flags, context):
        matches.append((self.extractors[index], (start, end)))

    self.hyperscan_db.scan(text_bytes, match_event_handler=on_match)

    # Build a lookup table of byte offset -&gt; str offset for all of the
    # matches we found. Stepping through offsets in sorted order avoids
    # having to decode each part of the string more than once:
    byte_to_str_offset = {}
    last_byte_offset = 0
    str_offset = 0
    byte_offsets = sorted(set(i for m in matches for i in m[1]))
    for byte_offset in byte_offsets:
        try:
            str_offset += len(
                text_bytes[last_byte_offset:byte_offset].decode(&#34;utf8&#34;)
            )
        except UnicodeDecodeError:
            # offsets will fail to decode for invalid regex matches
            # that don&#39;t align with a unicode character
            continue
        byte_to_str_offset[byte_offset] = str_offset
        last_byte_offset = byte_offset

    # Narrow down our matches to only those that successfully decoded,
    # re-run regex against just the matching strings to get match groups
    # (which aren&#39;t provided by hyperscan), and tokenize:
    for extractor, (start, end) in matches:
        if start in byte_to_str_offset and end in byte_to_str_offset:
            start = byte_to_str_offset[start]
            end = byte_to_str_offset[end]
            m = extractor.compiled_regex.match(text[start:end])
            yield extractor.get_token(m, offset=start)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="eyecite.tokenizers.Tokenizer" href="#eyecite.tokenizers.Tokenizer">Tokenizer</a></b></code>:
<ul class="hlist">
<li><code><a title="eyecite.tokenizers.Tokenizer.append_text" href="#eyecite.tokenizers.Tokenizer.append_text">append_text</a></code></li>
<li><code><a title="eyecite.tokenizers.Tokenizer.get_extractors" href="#eyecite.tokenizers.Tokenizer.get_extractors">get_extractors</a></code></li>
<li><code><a title="eyecite.tokenizers.Tokenizer.tokenize" href="#eyecite.tokenizers.Tokenizer.tokenize">tokenize</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="eyecite.tokenizers.Tokenizer"><code class="flex name class">
<span>class <span class="ident">Tokenizer</span></span>
<span>(</span><span>extractors: List[<a title="eyecite.models.TokenExtractor" href="models.html#eyecite.models.TokenExtractor">TokenExtractor</a>] = &lt;factory&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>A tokenizer takes a list of extractors, and provides a tokenize()
method to tokenize text using those extractors.
This base class should be overridden by tokenizers that use a
more efficient strategy for running all the extractors.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class Tokenizer:
    &#34;&#34;&#34;A tokenizer takes a list of extractors, and provides a tokenize()
    method to tokenize text using those extractors.
    This base class should be overridden by tokenizers that use a
    more efficient strategy for running all the extractors.&#34;&#34;&#34;

    extractors: List[TokenExtractor] = field(
        default_factory=lambda: list(EXTRACTORS)
    )

    def tokenize(self, text: str) -&gt; Tuple[Tokens, List[Tuple[int, Token]]]:
        &#34;&#34;&#34;Tokenize text and return list of all tokens, followed by list of
        just non-string tokens along with their positions in the first list.&#34;&#34;&#34;
        # Sort all matches by start offset ascending, then end offset
        # descending. Remove overlaps by returning only matches
        # where the current start offset is greater than the previously
        # returned end offset. Also return text between matches.
        citation_tokens = []
        all_tokens: Tokens = []
        tokens = sorted(
            self.extract_tokens(text), key=lambda m: (m.start, -m.end)
        )
        last_token = None
        offset = 0
        for token in tokens:
            if last_token:
                # Sometimes the exact same cite is matched by two different
                # regexes. Attempt to merge rather than discarding one or the
                # other:
                merged = last_token.merge(token)
                if merged:
                    continue
            if offset &gt; token.start:
                # skip overlaps
                continue
            if offset &lt; token.start:
                # capture plain text before each match
                self.append_text(all_tokens, text[offset : token.start])
            # capture match
            citation_tokens.append((len(all_tokens), token))
            all_tokens.append(token)
            offset = token.end
            last_token = token
        # capture plain text after final match
        if offset &lt; len(text):
            self.append_text(all_tokens, text[offset:])
        return all_tokens, citation_tokens

    def get_extractors(self, text: str):
        &#34;&#34;&#34;Subclasses can override this to filter extractors based on text.&#34;&#34;&#34;
        return self.extractors

    def extract_tokens(self, text) -&gt; Generator[Token, None, None]:
        &#34;&#34;&#34;Get all instances where an extractor matches the given text.&#34;&#34;&#34;
        for extractor in self.get_extractors(text):
            for match in extractor.get_matches(text):
                yield extractor.get_token(match)

    @staticmethod
    def append_text(tokens, text):
        &#34;&#34;&#34;Split text into words, treating whitespace as a word, and append
        to tokens. NOTE this is a significant portion of total runtime of
        get_citations(), so benchmark if changing.
        &#34;&#34;&#34;
        for part in text.split(&#34; &#34;):
            if part:
                tokens.extend((part, &#34; &#34;))
            else:
                tokens.append(&#34; &#34;)
        tokens.pop()  # remove final extra space</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="eyecite.tokenizers.AhocorasickTokenizer" href="#eyecite.tokenizers.AhocorasickTokenizer">AhocorasickTokenizer</a></li>
<li><a title="eyecite.tokenizers.HyperscanTokenizer" href="#eyecite.tokenizers.HyperscanTokenizer">HyperscanTokenizer</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="eyecite.tokenizers.Tokenizer.extractors"><code class="name">var <span class="ident">extractors</span> : List[<a title="eyecite.models.TokenExtractor" href="models.html#eyecite.models.TokenExtractor">TokenExtractor</a>]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="eyecite.tokenizers.Tokenizer.append_text"><code class="name flex">
<span>def <span class="ident">append_text</span></span>(<span>tokens, text)</span>
</code></dt>
<dd>
<div class="desc"><p>Split text into words, treating whitespace as a word, and append
to tokens. NOTE this is a significant portion of total runtime of
get_citations(), so benchmark if changing.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def append_text(tokens, text):
    &#34;&#34;&#34;Split text into words, treating whitespace as a word, and append
    to tokens. NOTE this is a significant portion of total runtime of
    get_citations(), so benchmark if changing.
    &#34;&#34;&#34;
    for part in text.split(&#34; &#34;):
        if part:
            tokens.extend((part, &#34; &#34;))
        else:
            tokens.append(&#34; &#34;)
    tokens.pop()  # remove final extra space</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="eyecite.tokenizers.Tokenizer.extract_tokens"><code class="name flex">
<span>def <span class="ident">extract_tokens</span></span>(<span>self, text) ‑> Generator[<a title="eyecite.models.Token" href="models.html#eyecite.models.Token">Token</a>, None, None]</span>
</code></dt>
<dd>
<div class="desc"><p>Get all instances where an extractor matches the given text.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_tokens(self, text) -&gt; Generator[Token, None, None]:
    &#34;&#34;&#34;Get all instances where an extractor matches the given text.&#34;&#34;&#34;
    for extractor in self.get_extractors(text):
        for match in extractor.get_matches(text):
            yield extractor.get_token(match)</code></pre>
</details>
</dd>
<dt id="eyecite.tokenizers.Tokenizer.get_extractors"><code class="name flex">
<span>def <span class="ident">get_extractors</span></span>(<span>self, text: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Subclasses can override this to filter extractors based on text.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_extractors(self, text: str):
    &#34;&#34;&#34;Subclasses can override this to filter extractors based on text.&#34;&#34;&#34;
    return self.extractors</code></pre>
</details>
</dd>
<dt id="eyecite.tokenizers.Tokenizer.tokenize"><code class="name flex">
<span>def <span class="ident">tokenize</span></span>(<span>self, text: str) ‑> Tuple[List[Union[<a title="eyecite.models.Token" href="models.html#eyecite.models.Token">Token</a>, str]], List[Tuple[int, <a title="eyecite.models.Token" href="models.html#eyecite.models.Token">Token</a>]]]</span>
</code></dt>
<dd>
<div class="desc"><p>Tokenize text and return list of all tokens, followed by list of
just non-string tokens along with their positions in the first list.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tokenize(self, text: str) -&gt; Tuple[Tokens, List[Tuple[int, Token]]]:
    &#34;&#34;&#34;Tokenize text and return list of all tokens, followed by list of
    just non-string tokens along with their positions in the first list.&#34;&#34;&#34;
    # Sort all matches by start offset ascending, then end offset
    # descending. Remove overlaps by returning only matches
    # where the current start offset is greater than the previously
    # returned end offset. Also return text between matches.
    citation_tokens = []
    all_tokens: Tokens = []
    tokens = sorted(
        self.extract_tokens(text), key=lambda m: (m.start, -m.end)
    )
    last_token = None
    offset = 0
    for token in tokens:
        if last_token:
            # Sometimes the exact same cite is matched by two different
            # regexes. Attempt to merge rather than discarding one or the
            # other:
            merged = last_token.merge(token)
            if merged:
                continue
        if offset &gt; token.start:
            # skip overlaps
            continue
        if offset &lt; token.start:
            # capture plain text before each match
            self.append_text(all_tokens, text[offset : token.start])
        # capture match
        citation_tokens.append((len(all_tokens), token))
        all_tokens.append(token)
        offset = token.end
        last_token = token
    # capture plain text after final match
    if offset &lt; len(text):
        self.append_text(all_tokens, text[offset:])
    return all_tokens, citation_tokens</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="eyecite" href="index.html">eyecite</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="eyecite.tokenizers.AhocorasickTokenizer" href="#eyecite.tokenizers.AhocorasickTokenizer">AhocorasickTokenizer</a></code></h4>
<ul class="">
<li><code><a title="eyecite.tokenizers.AhocorasickTokenizer.get_extractors" href="#eyecite.tokenizers.AhocorasickTokenizer.get_extractors">get_extractors</a></code></li>
<li><code><a title="eyecite.tokenizers.AhocorasickTokenizer.make_ahocorasick_filter" href="#eyecite.tokenizers.AhocorasickTokenizer.make_ahocorasick_filter">make_ahocorasick_filter</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="eyecite.tokenizers.HyperscanTokenizer" href="#eyecite.tokenizers.HyperscanTokenizer">HyperscanTokenizer</a></code></h4>
<ul class="">
<li><code><a title="eyecite.tokenizers.HyperscanTokenizer.cache_dir" href="#eyecite.tokenizers.HyperscanTokenizer.cache_dir">cache_dir</a></code></li>
<li><code><a title="eyecite.tokenizers.HyperscanTokenizer.extract_tokens" href="#eyecite.tokenizers.HyperscanTokenizer.extract_tokens">extract_tokens</a></code></li>
<li><code><a title="eyecite.tokenizers.HyperscanTokenizer.hyperscan_db" href="#eyecite.tokenizers.HyperscanTokenizer.hyperscan_db">hyperscan_db</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="eyecite.tokenizers.Tokenizer" href="#eyecite.tokenizers.Tokenizer">Tokenizer</a></code></h4>
<ul class="">
<li><code><a title="eyecite.tokenizers.Tokenizer.append_text" href="#eyecite.tokenizers.Tokenizer.append_text">append_text</a></code></li>
<li><code><a title="eyecite.tokenizers.Tokenizer.extract_tokens" href="#eyecite.tokenizers.Tokenizer.extract_tokens">extract_tokens</a></code></li>
<li><code><a title="eyecite.tokenizers.Tokenizer.extractors" href="#eyecite.tokenizers.Tokenizer.extractors">extractors</a></code></li>
<li><code><a title="eyecite.tokenizers.Tokenizer.get_extractors" href="#eyecite.tokenizers.Tokenizer.get_extractors">get_extractors</a></code></li>
<li><code><a title="eyecite.tokenizers.Tokenizer.tokenize" href="#eyecite.tokenizers.Tokenizer.tokenize">tokenize</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>